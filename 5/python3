-5.0,51.17416241104695,-0.8851839706364495
-4.974937343358396,48.96669632887266,-0.8835280902619664
-4.949874686716792,46.67922296549196,-0.8818823172724164
-4.924812030075188,44.317648269670144,-0.8802465930582715
-4.899749373433584,41.888030947483855,-0.8786208593113471
-4.87468671679198,39.39656775628154,-0.8770050580237418
-4.849624060150376,36.84957847026495,-0.8753991314867746
-4.824561403508772,34.25349055472209,-0.8738030222899188
-4.799498746867168,31.614823586670276,-0.8722166733197327
-4.774436090225564,28.94017346030038,-0.8706400277587889
-4.74937343358396,26.236196416147006,-0.8690730290846003
-4.724310776942356,23.509592933351026,-0.8675156210685437
-4.6992481203007515,20.76709152472189,-0.8659677477747818
-4.674185463659148,18.015432474546763,-0.864429353559182
-4.649122807017544,15.261351559238307,-0.8629003830682338
-4.62406015037594,12.511563790955467,-0.8613807812379648
-4.598997493734336,9.77274722427054,-0.8598704932928534
-4.5739348370927315,7.051526865803266,-0.8583694647447409
-4.548872180451128,4.354458726480171,-0.856877641391742
-4.523809523809524,1.6880140557244232,-0.8553949693171533
-4.49874686716792,-0.9414362035728807,-0.8539213948883602
-4.473684210526316,-3.527636701003612,-0.8524568647557424
-4.448621553884712,-6.064462667861111,-0.8510013258515805
-4.423558897243108,-8.545934572717385,-0.8495547253889569
-4.398496240601504,-10.966232407279985,-0.8481170108606599
-4.3734335839598995,-13.31970956737229,-0.846688130038085
-4.348370927318296,-15.600906294894152,-0.8452680309701347
-4.323308270676692,-17.80456264771309,-0.8438566619821195
-4.298245614035087,-19.92563096561438,-0.8424539716746564
-4.273182957393484,-21.959287801698753,-0.8410599089225675
-4.2481203007518795,-23.900945289947483,-0.8396744228737784
-4.223057644110276,-25.746261921082574,-0.8382974629482149
-4.197994987468672,-27.491152700328172,-0.836928978836702
-4.172932330827067,-29.13179866221799,-0.835568920499859
-4.147869674185464,-30.66465571920181,-0.8342172381669978
-4.12280701754386,-32.086462822463496,-0.8328738823350184
-4.097744360902256,-33.39424941507938,-0.831538803767307
-4.072681704260652,-34.585342159413166,-0.8302119534926313
-4.0476190476190474,-35.65737092245083,-0.8288932828040385
-4.022556390977444,-36.60827400463312,-0.8275827432577517
-3.9974937343358397,-37.43630259962776,-0.8262802866720672
-3.9724310776942358,-38.14002447440262,-0.8249858651262527
-3.947368421052632,-38.71832686090552,-0.8236994309594452
-3.9223057644110275,-39.170418552621406,-0.8224209367695492
-3.8972431077694236,-39.49583120126105,-0.821150335412136
-3.8721804511278197,-39.69441981082875,-0.819887579999344
-3.8471177944862154,-39.766362428317436,-0.8186326238987782
-3.8220551378446115,-39.71215903228193,-0.8173854207324122
-3.7969924812030076,-39.5326296225396,-0.8161459243754895
-3.7719298245614037,-39.22891151623874,-0.8149140889554269
-3.7468671679198,-38.80245585751123,-0.8136898688507181
-3.7218045112781954,-38.2550233498857,-0.8124732186898387
-3.6967418546365916,-37.58867922257198,-0.8112640933501511
-3.6716791979949877,-36.80578744363575,-0.8100624479568133
-3.6466165413533833,-35.90900419495643,-0.8088682378816852
-3.6215538847117794,-34.9012706256986,-0.8076814187422392
-3.5964912280701755,-33.78580490282189,-0.8065019464004701
-3.5714285714285716,-32.566093578902596,-0.805329776961808
-3.5463659147869677,-31.245882299237795,-0.8041648667740305
-3.5213032581453634,-29.829165871843934,-0.8030071724261789
-3.4962406015037595,-28.32017772554553,-0.8018566507474734
-3.4711779448621556,-26.723378782867837,-0.8007132588062316
-3.4461152882205512,-25.04344577590044,-0.7995769539087878
-3.4210526315789473,-23.285259034679743,-0.7984476935984148
-3.3959899749373434,-21.453889778945165,-0.7973254356542454
-3.3709273182957395,-19.554586945354743,-0.7962101380901981
-3.3458646616541357,-17.59276358339391,-0.7951017591539034
-3.3208020050125313,-15.573982854277684,-0.7940002573256313
-3.2957393483709274,-13.50394366812645,-0.7929055913172225
-3.2706766917293235,-11.38846599558579,-0.7918177200710191
-3.245614035087719,-9.23347589086284,-0.7907366027587999
-3.2205513784461153,-7.044990263858278,-0.7896621987807154
-3.1954887218045114,-4.829101439686423,-0.7885944677642265
-3.1704260651629075,-2.5919615443942003,-0.7875333695630439
-3.1453634085213036,-0.33976675610854495,-0.7864788642560714
-3.1203007518796992,1.9212585388358687,-0.7854309121463495
-3.0952380952380953,4.1848776450140805,-0.7843894737600022
-3.0701754385964914,6.444857472376981,-0.7833545098451862
-3.045112781954887,8.694984426145384,-0.7823259813710415
-3.020050125313283,10.929080232289538,-0.7813038495266456
-2.9949874686716793,13.141017658928579,-0.7802880757199686
-2.9699248120300754,15.324736094259517,-0.7792786215768319
-2.9448621553884715,17.474256942001787,-0.7782754489398687
-2.919799498746867,19.58369879581511,-0.7772785198674869
-2.8947368421052633,21.64729235471762,-0.7762877966328345
-2.8696741854636594,23.65939504219618,-0.7753032417227688
-2.844611528822055,25.61450529245699,-0.7743248178368253
-2.819548872180451,27.507276468114778,-0.7733524878861919
-2.7944862155388472,29.332530374557052,-0.772386214992685
-2.7694235588972433,31.085270337244356,-0.7714259624877269
-2.7443609022556394,32.760693809318084,-0.7704716939113274
-2.719298245614035,34.35420447807811,-0.7695233730110681
-2.694235588972431,35.861423840162374,-0.7685809637410878
-2.6691729323308273,37.278202216605855,-0.7676444302610723
-2.644110275689223,38.60062918037329,-0.7667137369352466
-2.619047619047619,39.82504337044553,-0.7657888483313691
-2.593984962406015,40.94804166809031,-0.7648697292197294
-2.5689223057644113,41.96648771255802,-0.7639563445721491
-2.5438596491228074,42.87751973511213,-0.7630486595609843
-2.518796992481203,43.678557692023325,-0.7621466395581324
-2.493734335839599,44.36730967892503,-0.761250250134041
-2.4686716791979952,44.941777610741,-0.7603594570567195
-2.443609022556391,45.40026215324568,-0.7594742262907543
-2.418546365914787,45.74136689420574,-0.7585945239963271
-2.393483709273183,45.96400174396556,-0.7577203165282348
-2.368421052631579,46.06738555728142,-0.7568515704349144
-2.3433583959899753,46.05104797016908,-0.7559882524574688
-2.318295739348371,45.91483044750652,-0.755130329528698
-2.293233082706767,45.65888653911945,-0.7542777687721309
-2.268170426065163,45.28368134406869,-0.7534305375010621
-2.243107769423559,44.789990184851035,-0.752588603217591
-2.218045112781955,44.17889649521161,-0.7517519336116633
-2.192982456140351,43.45178892724352,-0.7509204965601173
-2.167919799498747,42.61035768541302,-0.750094260125732
-2.1428571428571432,41.656590097091566,-0.7492731925562788
-2.117794486215539,40.592765431094286,-0.7484572622835763
-2.092731829573935,39.421448977613956,-0.7476464379225483
-2.067669172932331,38.14548540479417,-0.7468406882702849
-2.0426065162907268,36.767991409002036,-0.7460399823051072
-2.017543859649123,35.292347677633614,-0.7452442891856346
-1.992481203007519,33.72219018501027,-0.7444535782498555
-1.967418546365915,32.06140084359814,-0.7436678190142019
-1.9423558897243112,30.314097534398865,-0.7428869811726264
-1.9172932330827068,28.484623541916893,-0.7421110345956827
-1.892230576441103,26.57753642060134,-0.7413399493296096
-1.867167919799499,24.59759632108357,-0.740573695595418
-1.8421052631578947,22.549753805886347,-0.7398122437879814
-1.8170426065162908,20.439137185556856,-0.7390555644751295
-1.791979949874687,18.271039407376126,-0.7383036283967458
-1.766917293233083,16.0509045299167,-0.7375564064638669
-1.741854636591479,13.784313817754235,-0.7368138697577867
-1.7167919799498748,11.47697149158747,-0.7360759895291639
-1.6917293233082709,9.13469016988045,-0.7353427371971313
-1.666666666666667,6.763376038908277,-0.7346140843484101
-1.6416040100250626,4.369013788764453,-0.7338900027364269
-1.6165413533834587,1.9576513534673556,-0.7331704642804338
-1.5914786967418548,-0.4646155062121098,-0.732455441064632
-1.566416040100251,-2.8916587381910754,-0.7317449053372996
-1.541353383458647,-5.3173336134854505,-0.7310388295099213
-1.5162907268170427,-7.735494380101857,-0.7303371861563225
-1.4912280701754388,-10.14000991683123,-0.7296399480118065
-1.466165413533835,-12.524779347374528,-0.7289470879722952
-1.4411027568922306,-14.883747575338191,-0.7282585790934729
-1.4160401002506267,-17.210920700846348,-0.7275743945899337
-1.3909774436090228,-19.50038127982349,-0.7268945078343323
-1.3659147869674189,-21.74630338740518,-0.7262188923565382
-1.340852130325815,-23.94296744743756,-0.7255475218427931
-1.3157894736842106,-26.084774790623197,-0.7248803701348722
-1.2907268170426067,-28.16626190456267,-0.7242174112292477
-1.2656641604010028,-30.182114339726212,-0.7235586192762576
-1.2406015037593985,-32.12718023626325,-0.7229039685792764
-1.2155388471177946,-33.99648343752163,-0.7222534335938895
-1.1904761904761907,-35.78523615719714,-0.7216069889270711
-1.1654135338345868,-37.488851168164764,-0.7209646093363661
-1.140350877192983,-39.102953482256964,-0.7203262697290744
-1.1152882205513786,-40.623391491542655,-0.7196919451614392
-1.0902255639097747,-42.04624754302439,-0.7190616108378388
-1.0651629072681708,-43.36784792010606,-0.7184352421099812
-1.0401002506265664,-44.584772205683215,-0.717812814476103
-1.0150375939849625,-45.693862003273395,-0.7171943035801704
-0.9899749373433586,-46.69222899422719,-0.7165796852110856
-0.9649122807017543,-47.577262310738746,-0.7159689353018942
-0.9398496240601508,-48.346635206104445,-0.715362029928998
-0.9147869674185465,-48.998311005454724,-0.7147589453113701
-0.889724310776943,-49.53054832200119,-0.7141596578097736
-0.8646616541353387,-49.94190552569802,-0.7135641439259841
-0.8395989974937343,-50.231244453104054,-0.7129723803020152
-0.8145363408521309,-50.39773334914893,-0.7123843437193469
-0.7894736842105265,-50.44084903344632,-0.7118000110981589
-0.7644110275689222,-50.3603782857552,-0.711219359496566
-0.7393483709273188,-50.1564184471617,-0.7106423661098565
-0.7142857142857144,-49.82937723553402,-0.7100690082697358
-0.6892230576441101,-49.37997177578608,-0.7094992634435716
-0.6641604010025066,-48.809226847467954,-0.708933109233643
-0.6390977443609023,-48.11847235417551,-0.7083705233763938
-0.6140350877192988,-47.309340021236984,-0.7078114837416878
-0.5889724310776945,-46.38375933007997,-0.707255968332068
-0.5639097744360901,-45.34395269960988,-0.7067039552820201
-0.5388471177944867,-44.19242992683004,-0.7061554228572378
-0.5137844611528823,-42.93198190080214,-0.7056103494538921
-0.4887218045112789,-41.56567360588071,-0.7050687135979044
-0.46365914786967455,-40.09683643194563,-0.7045304939442222
-0.4385964912280702,-38.52905981110787,-0.7039956692760985
-0.41353383458646675,-36.866182202059896,-0.7034642185043745
-0.3884711779448624,-35.11228144488924,-0.7029361206667657
-0.36340852130325807,-33.271664510762,-0.7024113549271506
-0.3383458646616546,-31.34885667240734,-0.7018899005748641
-0.3132832080200503,-29.34859012279672,-0.7013717370239926
-0.28822055137844593,-27.27579207080315,-0.7008568438126738
-0.2631578947368425,-25.1355723439433,-0.7003452006023986
-0.23809523809523814,-22.93321052954972,-0.6998367871773171
-0.21303258145363468,-20.674142686885077,-0.6993315834435476
-0.18796992481203034,-18.363947663789403,-0.6988295694284888
-0.162907268170426,-16.00833305245283,-0.6983307252801353
-0.13784461152882255,-13.61312081981214,-0.6978350312663961
-0.1127819548872182,-11.184232648892575,-0.6973424677744173
-0.08771929824561475,-8.72767502814541,-0.6968530153099064
-0.06265664160401041,-6.249524126464179,-0.6963666544964616
-0.03759398496240607,-3.755910492107649,-0.6958833660749029
-0.012531328320802615,-1.2530036141968723,-0.6954031309026072
0.012531328320801727,1.2530036141967835,-0.6949259299528465
0.03759398496240607,3.755910492107649,-0.6944517443141289
0.06265664160400952,6.249524126464093,-0.6939805551895437
0.08771929824561386,8.727675028145322,-0.6935123438961082
0.1127819548872182,11.184232648892575,-0.6930470918641191
0.13784461152882166,13.613120819812053,-0.6925847806365063
0.162907268170426,16.00833305245283,-0.6921253918681903
0.18796992481202945,18.363947663789318,-0.6916689073254423
0.2130325814536338,20.674142686885,-0.6912153088852475
0.23809523809523814,22.93321052954972,-0.6907645785346721
0.2631578947368416,25.13557234394322,-0.6903166983702326
0.28822055137844593,27.27579207080315,-0.6898716505972688
0.3132832080200494,29.34859012279664,-0.6894294175293197
0.33834586466165373,31.348856672407273,-0.6889899815875026
0.36340852130325807,33.271664510762,-0.688553325299895
0.3884711779448615,35.11228144488918,-0.6881194313009198
0.41353383458646586,36.86618220205983,-0.6876882823307341
0.4385964912280702,38.52905981110787,-0.6872598612346195
0.46365914786967366,40.096836431945576,-0.6868341509623773
0.488721804511278,41.56567360588066,-0.6864111345677257
0.5137844611528823,42.93198190080214,-0.6859907952077001
0.5388471177944858,44.19242992683,-0.6855731161420567
0.5639097744360901,45.34395269960988,-0.6851580807326788
0.5889724310776936,46.383759330079926,-0.6847456724429866
0.6140350877192979,47.30934002123697,-0.6843358748373491
0.6390977443609023,48.11847235417551,-0.6839286715805004
0.6641604010025057,48.809226847467926,-0.6835240464369576
0.6892230576441101,49.37997177578608,-0.6831219832704419
0.7142857142857135,49.829377235534004,-0.6827224660433039
0.7393483709273179,50.15641844716169,-0.68232547881595
0.7644110275689222,50.3603782857552,-0.6819310057462731
0.7894736842105257,50.44084903344633,-0.6815390310890858
0.81453634085213,50.39773334914893,-0.681149539195556
0.8395989974937343,50.231244453104054,-0.6807625145126468
0.8646616541353378,49.94190552569803,-0.6803779415825576
0.8897243107769421,49.5305483220012,-0.679995805042169
0.9147869674185465,48.998311005454724,-0.679616089622491
0.9398496240601499,48.34663520610447,-0.6792387801481126
0.9649122807017543,47.577262310738746,-0.678863861536656
0.9899749373433577,46.69222899422723,-0.6784913187982327
1.015037593984962,45.69386200327341,-0.678121137034902
1.0401002506265664,44.584772205683215,-0.6777533014401337
1.0651629072681699,43.36784792010611,-0.677387797298272
1.0902255639097742,42.046247543024414,-0.6770246099840038
1.1152882205513777,40.6233914915427,-0.6766637249618284
1.140350877192982,39.10295348225702,-0.6763051277855306
1.1654135338345863,37.48885116816479,-0.6759488040976572
1.1904761904761898,35.78523615719721,-0.675594739628995
1.2155388471177941,33.996483437521675,-0.6752429201980522
1.2406015037593985,32.12718023626325,-0.674893331710543
1.265664160401002,30.18211433972628,-0.674545960158874
1.2907268170426063,28.166261904562706,-0.674200791621634
1.3157894736842106,26.084774790623197,-0.6738578122630863
1.340852130325814,23.94296744743764,-0.6735170083326635
1.3659147869674184,21.74630338740522,-0.6731783661644652
1.3909774436090219,19.500381279823568,-0.6728418721767591
1.4160401002506262,17.210920700846387,-0.6725075128714825
1.4411027568922306,14.883747575338191,-0.6721752748337495
1.466165413533834,12.524779347374611,-0.6718451447313587
1.4912280701754383,10.140009916831273,-0.6715171093143041
1.5162907268170418,7.735494380101942,-0.6711911554142889
1.5413533834586461,5.317333613485537,-0.6708672699442415
1.5664160401002505,2.8916587381911185,-0.6705454398978348
1.591478696741854,0.4646155062121955,-0.6702256523490069
1.6165413533834583,-1.957651353467313,-0.6699078944514858
1.6416040100250626,-4.369013788764453,-0.6695921534383156
1.666666666666666,-6.763376038908192,-0.6692784166213858
1.6917293233082704,-9.134690169880407,-0.6689666713909626
1.7167919799498748,-11.47697149158747,-0.668656905215224
1.7418546365914782,-13.784313817754157,-0.6683491056397957
1.7669172932330826,-16.050904529916664,-0.6680432602872908
1.791979949874686,-18.27103940737605,-0.6677393568568515
1.8170426065162903,-20.439137185556817,-0.6674373831236936
1.8421052631578947,-22.549753805886347,-0.667137326938653
1.8671679197994981,-24.597596321083504,-0.666839176227735
1.8922305764411025,-26.5775364206013,-0.6665429189916664
1.917293233082706,-28.484623541916825,-0.6662485433054491
1.9423558897243103,-30.314097534398794,-0.6659560373179174
1.9674185463659146,-32.061400843598115,-0.6656653892512965
1.992481203007518,-33.72219018501021,-0.6653765874007643
2.0175438596491224,-35.292347677633586,-0.6650896201340153
2.0426065162907268,-36.767991409002036,-0.6648044758908269
2.06766917293233,-38.14548540479412,-0.6645211431826279
2.0927318295739346,-39.421448977613935,-0.66423961059207
2.117794486215539,-40.592765431094286,-0.6639598667726011
2.1428571428571423,-41.656590097091545,-0.6636819004480408
2.1679197994987467,-42.610357685413,-0.6634057004121588
2.19298245614035,-43.45178892724349,-0.663131255528256
2.2180451127819545,-44.17889649521159,-0.6628585547287462
2.243107769423559,-44.789990184851035,-0.6625875870147425
2.2681704260651623,-45.283681344068675,-0.6623183414556437
2.2932330827067666,-45.65888653911946,-0.6620508071887247
2.31829573934837,-45.914830447506525,-0.6617849734187283
2.3433583959899744,-46.05104797016907,-0.66152082941746
2.3684210526315788,-46.06738555728142,-0.6612583645233842
2.393483709273182,-45.96400174396557,-0.6609975681412226
2.4185463659147866,-45.74136689420574,-0.6607384297415565
2.443609022556391,-45.40026215324568,-0.6604809388604291
2.4686716791979944,-44.94177761074102,-0.6602250850989518
2.4937343358395987,-44.367309678925054,-0.6599708581229115
2.518796992481203,-43.678557692023325,-0.6597182476623804
2.5438596491228065,-42.87751973511217,-0.6594672435113293
2.568922305764411,-41.966487712558035,-0.6592178355272407
2.5939849624060143,-40.94804166809034,-0.6589700136307259
2.6190476190476186,-39.82504337044555,-0.6587237678051439
2.644110275689223,-38.60062918037329,-0.6584790880962217
2.6691729323308264,-37.278202216605905,-0.6582359646116782
2.6942355889724308,-35.86142384016239,-0.6579943875208483
2.719298245614034,-34.35420447807817,-0.6577543470543108
2.7443609022556386,-32.760693809318134,-0.6575158335035178
2.769423558897243,-31.08527033724438,-0.6572788372204259
2.7944862155388464,-29.33253037455712,-0.65704334861713
2.8195488721804507,-27.50727646811481,-0.6568093581654986
2.844611528822055,-25.61450529245699,-0.6565768563968121
2.8696741854636585,-23.659395042196252,-0.656345833901402
2.894736842105263,-21.647292354717656,-0.6561162813282934
2.919799498746867,-19.58369879581511,-0.6558881893848479
2.9448621553884706,-17.474256942001865,-0.6556615488364108
2.969924812030075,-15.324736094259558,-0.6554363505059578
2.9949874686716784,-13.141017658928657,-0.6552125852737459
3.0200501253132828,-10.929080232289577,-0.6549902440769648
3.045112781954886,-8.694984426145462,-0.654769317909391
3.0701754385964914,-6.444857472376981,-0.6545497978210439
3.095238095238095,-4.1848776450141205,-0.6543316749178434
3.1203007518796984,-1.9212585388359484,-0.65411494036127
3.1453634085213036,0.33976675610854495,-0.6538995853680267
3.170426065162907,2.591961544394161,-0.6536856012097024
3.1954887218045105,4.829101439686343,-0.6534729792124379
3.220551378446114,7.04499026385816,-0.6532617107565935
3.245614035087719,9.23347589086284,-0.6530517872764184
3.2706766917293226,11.388465995585717,-0.6528432002597224
3.295739348370926,13.50394366812634,-0.6526359412475491
3.3208020050125313,15.573982854277684,-0.6524300018338508
3.3458646616541348,17.592763583393836,-0.6522253736651663
3.370927318295738,19.554586945354643,-0.6520220484402993
3.3959899749373434,21.453889778945165,-0.6518200179099995
3.421052631578947,23.28525903467971,-0.6516192738766453
3.4461152882205504,25.04344577590038,-0.6514198081939281
3.4711779448621556,26.723378782867837,-0.6512216127665387
3.496240601503759,28.320177725545502,-0.6510246795498558
3.5213032581453625,29.829165871843877,-0.6508290005496356
3.5463659147869677,31.245882299237795,-0.6506345678217037
3.571428571428571,32.56609357890257,-0.6504413734716489
3.5964912280701746,33.78580490282184,-0.650249409654518
3.62155388471178,34.90127062569862,-0.6500586685745137
3.6466165413533833,35.90900419495643,-0.6498691424846926
3.6716791979949868,36.80578744363572,-0.649680823686667
3.69674185463659,37.58867922257194,-0.6494937045303061
3.7218045112781954,38.2550233498857,-0.6493077774134407
3.746867167919799,38.802455857511205,-0.6491230347815695
3.7719298245614024,39.22891151623872,-0.648939469127566
3.7969924812030076,39.5326296225396,-0.6487570729913876
3.822055137844611,39.71215903228193,-0.6485758389597879
3.8471177944862145,39.76636242831744,-0.6483957596660279
3.8721804511278197,39.69441981082875,-0.6482168277895912
3.897243107769423,39.495831201261055,-0.6480390360559
3.9223057644110266,39.17041855262143,-0.6478623772360323
3.947368421052632,38.71832686090552,-0.6476868441464423
3.9724310776942353,38.14002447440265,-0.6475124296486802
3.9974937343358388,37.43630259962779,-0.6473391266491156
4.022556390977442,36.60827400463318,-0.6471669280986619
4.0476190476190474,35.65737092245083,-0.6469958269925018
4.072681704260651,34.5853421594132,-0.6468258163698148
4.097744360902254,33.39424941507947,-0.6466568893135072
4.12280701754386,32.086462822463496,-0.6464890389499415
4.147869674185463,30.664655719201868,-0.6463222584486695
4.1729323308270665,29.131798662218042,-0.6461565410221668
4.197994987468672,27.491152700328172,-0.6459918799255667
4.223057644110275,25.74626192108264,-0.6458282684563987
4.248120300751879,23.900945289947547,-0.6456656999543265
4.273182957393484,21.959287801698753,-0.6455041678008882
4.298245614035087,19.92563096561438,-0.6453436654192375
4.323308270676691,17.804562647713162,-0.6451841862738881
4.348370927318296,15.600906294894152,-0.6450257238704572
4.3734335839598995,13.31970956737229,-0.6448682717554126
4.398496240601503,10.966232407280074,-0.6447118235158198
4.423558897243108,8.545934572717385,-0.6445563727790918
4.448621553884712,6.064462667861111,-0.6444019132127398
4.473684210526315,3.5276367010037006,-0.6442484385241245
4.4987468671679185,0.9414362035730584,-0.6440959424602115
4.523809523809524,-1.6880140557244232,-0.6439444188073247
4.548872180451127,-4.354458726480075,-0.6437938613909041
4.573934837092731,-7.051526865803172,-0.6436442640752634
4.598997493734336,-9.77274722427054,-0.6434956207633497
4.624060150375939,-12.511563790955375,-0.6433479253965039
4.649122807017543,-15.26135155923821,-0.6432011719542239
4.674185463659148,-18.015432474546763,-0.6430553544539278
4.6992481203007515,-20.76709152472189,-0.6429104669507193
4.724310776942355,-23.509592933350927,-0.6427665035371544
4.74937343358396,-26.236196416147006,-0.6426234583430088
4.774436090225564,-28.94017346030038,-0.6424813255350483
4.799498746867167,-31.614823586670184,-0.6423400993167987
4.8245614035087705,-34.25349055472191,-0.6421997739283181
4.849624060150376,-36.84957847026495,-0.6420603436459706
4.874686716791979,-39.396567756281456,-0.6419218027822012
4.899749373433583,-41.888030947483685,-0.6417841456853117
4.924812030075188,-44.317648269670144,-0.6416473667392384
4.949874686716791,-46.67922296549188,-0.6415114603633311
4.974937343358395,-48.96669632887257,-0.6413764210121329
5.0,-51.17416241104695,-0.6412422431751621
#!/usr/bin/env python3
import csv
import sys
from typing import TypeVar

import numpy as np
import numpy.typing as npt
from matplotlib import pyplot as plt

NDArrayInt = npt.NDArray[np.intc]
NDArrayFloat = npt.NDArray[np.double]
ScalarOrArray = TypeVar("ScalarOrArray", float, np.double, NDArrayFloat)


def target_function(x: ScalarOrArray):
    return x ** 2 * np.sin(x) + 100 * np.sin(x) * np.cos(x)


# def target_function(x: ScalarOrArray) -> ScalarOrArray:
#     return np.sin(x * PARAMETERS[0]) + np.cos(x * PARAMETERS[1])  # type: ignore


def activation(x: ScalarOrArray) -> ScalarOrArray:
    ex = np.exp(x)
    return ex / (ex + 1)  # type: ignore


def activation_derivative(activation_of_x: ScalarOrArray) -> ScalarOrArray:
    return activation_of_x * (1 - activation_of_x)  # type: ignore


def loss(expected: ScalarOrArray, got: ScalarOrArray) -> ScalarOrArray:
    return np.square(got - expected)  # type: ignore


def loss_derivative(expected: ScalarOrArray, got: ScalarOrArray) -> ScalarOrArray:
    return (got - expected) * 2  # type: ignore


EVAL_XS = np.linspace(-5, 5, 400, dtype=np.double)
EVAL_YS = target_function(EVAL_XS)


class Network:
    def __init__(self, hidden_layer_size: int = 9) -> None:
        # NOTE: Hardcoded for 3 layers, and 1 dimension input and output
        self.hidden_layer_size = hidden_layer_size

        self.weights_hidden = np.random.uniform(
            -1.0, 1.0, size=(self.hidden_layer_size, 1)
        ).astype(np.double)
        self.weights_output = np.zeros(shape=(1, self.hidden_layer_size), dtype=np.double)
        self.biases_hidden = np.random.uniform(
            -1.0, 1.0, size=(self.hidden_layer_size, 1)
        ).astype(np.double)
        self.biases_output = np.zeros(shape=(1, 1))

    # Forward propagation stuff

    @staticmethod
    def forward(x: NDArrayFloat, weights: NDArrayFloat, biases: NDArrayFloat,
                apply_activation: bool = True) -> NDArrayFloat:
        out: NDArrayFloat = (weights @ x) + biases
        return activation(out) if apply_activation else out

    def predict(self, x: float) -> float:
        activations: NDArrayFloat = np.array([[x]], dtype=np.double)
        activations: NDArrayFloat = self.forward(activations, self.weights_hidden,
                                                 self.biases_hidden, True)
        activations: NDArrayFloat = self.forward(activations, self.weights_output,
                                                 self.biases_output, False)
        assert activations.size == 1
        return activations[0][0]

    def avg_loss(self) -> float:
        return loss(
            EVAL_YS,
            np.fromiter((self.predict(x) for x in EVAL_XS), dtype=np.double, count=len(EVAL_XS))
        ).mean()

    # Backwards propagation (learning)

    def train(self, train_inputs: NDArrayFloat, train_outputs: NDArrayFloat,
              epochs: int, mini_batch_size: int, learning_rate: float) -> None:
        """Train the network with SGD split into mini-batches"""

        # Instead of shuffling `inputs` and `outputs` simultaneously, we're
        # going to only keep track of indices into those arrays, and shuffle them instead.
        assert train_inputs.size == train_outputs.size
        train_indices: NDArrayInt = np.arange(len(train_inputs), dtype=np.intc)
        avg_loss_before: float = 0.0

        for i in range(epochs):
            # Loss calculations
            if i % 100 == 0:
                avg_loss_before = self.avg_loss()

            self.batch(train_inputs, train_outputs, train_indices, mini_batch_size, learning_rate)

            if i % 100 == 0:
                avg_loss_after = self.avg_loss()
                print(f"Epoch: {i} (Loss delta: {avg_loss_after - avg_loss_before})",
                      file=sys.stderr)

    def batch(self, train_inputs: NDArrayFloat, train_outputs: NDArrayFloat,
              train_indices: NDArrayInt, mini_batch_size: int, learning_rate: float) -> None:
        np.random.shuffle(train_indices)
        mini_batches = len(train_indices) // mini_batch_size

        for i in range(mini_batches):
            start = mini_batch_size * i
            end = start + mini_batch_size

            self.mini_batch(train_inputs, train_outputs, train_indices[start:end], learning_rate)

    def mini_batch(self, train_inputs: NDArrayFloat, train_outputs: NDArrayFloat,
                   train_indices: NDArrayInt, learning_rate: float) -> None:
        grad_weights_output: NDArrayFloat = np.zeros(shape=self.weights_output.shape,
                                                     dtype=np.double)
        grad_weights_hidden: NDArrayFloat = np.zeros(shape=self.weights_hidden.shape,
                                                     dtype=np.double)
        grad_biases_output: NDArrayFloat = np.zeros(shape=self.biases_output.shape,
                                                    dtype=np.double)
        grad_biases_hidden: NDArrayFloat = np.zeros(shape=self.biases_hidden.shape,
                                                    dtype=np.double)

        for i in train_indices:
            delta_wo, delta_wh, delta_bo, delta_bh = self.backprop(
                train_inputs[i],
                train_outputs[i],
            )

            grad_weights_output += delta_wo
            grad_weights_hidden += delta_wh
            grad_biases_output += delta_bo
            grad_biases_hidden += delta_bh

        proportion = learning_rate / len(train_indices)
        grad_weights_output *= proportion
        grad_weights_hidden *= proportion
        grad_biases_output *= proportion
        grad_biases_hidden *= proportion

        self.weights_output -= grad_weights_output
        self.weights_hidden -= grad_weights_hidden
        self.biases_output -= grad_biases_output
        self.biases_hidden -= grad_biases_hidden

    def backprop(self, input: float, expected_output: float) \
            -> tuple[NDArrayFloat, NDArrayFloat, NDArrayFloat, NDArrayFloat]:
        # Forward propagation
        activations_output_expected: NDArrayFloat = np.array([[expected_output]], dtype=np.double)
        activations_input: NDArrayFloat = np.array([[input]], dtype=np.double)

        activations_hidden: NDArrayFloat = activation(
            (self.weights_hidden @ activations_input) + self.biases_hidden
        )

        activations_output: NDArrayFloat = (self.weights_output @ activations_hidden) \
                                           + self.biases_output

        # Backwards propagation into the output layer
        delta_bo = loss_derivative(activations_output_expected, activations_output)
        delta_wo = delta_bo @ activations_hidden.T

        # Backpropagate into the hidden layer
        delta_bh = self.weights_output.T @ delta_bo
        delta_bh *= activation_derivative(activations_hidden)
        delta_wh = delta_bh @ activations_input

        return delta_wo, delta_wh, delta_bo, delta_bh


def main(hidden_layer_size: int = 13, epochs: int = 10_000, mini_batch_size: int = 100,
         learning_rate: float = 1e-1) -> None:
    # Teach the network
    train_xs = np.linspace(-5, 5, 10_000, dtype=np.double)
    train_ys = target_function(train_xs)

    net = Network(hidden_layer_size)
    net.train(train_xs, train_ys, epochs, mini_batch_size, learning_rate)

    for x, y in zip(EVAL_XS, EVAL_YS):
        print(x, y, net.predict(x), sep=",")


def plot():
    xs = np.array([], dtype=np.double)
    ys_real = np.array([], dtype=np.double)
    ys_predicted = np.array([], dtype=np.double)

    for row in csv.reader(sys.stdin):
        x, y_real, y_predicted = map(float, row)
        xs = np.append(xs, [x])
        ys_real = np.append(ys_real, [y_real])
        ys_predicted = np.append(ys_predicted, [y_predicted])

    avg_loss = loss(ys_real, ys_predicted).mean()
    print(f"Avg Loss: {avg_loss:.12f}")

    plt.plot(xs, ys_real, "r")
    plt.plot(xs, ys_predicted, "b")
    plt.legend(["Actual", "Prediction"])
    plt.xticks(np.arange(-5, 5, step=.5))
    # plt.yticks(np.arange(-2.2, 2.2, step=.2))
    plt.grid(True)
    plt.show()


if __name__ == "__main__":
    hidden_layer_size = 10
    epochs = 1000
    mini_batch_size = 100
    learning_rate = 0.1
    # main(hidden_layer_size, epochs, mini_batch_size, learning_rate)
    plot()
